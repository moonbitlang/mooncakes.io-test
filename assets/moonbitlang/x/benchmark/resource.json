{
  "kind": "package",
  "relative_path": "benchmark",
  "module_path": "moonbitlang/x",
  "readme_content": "# moonbitlang/x/benchmark\n\n## Overview\n\nThis section provides some benchmark APIs, which are unstable and may be removed or modified at any time.\n\n## Usage\n\nYou can create Criterion and add tasks(by `add` and `Task::new`) to them like this:\n\n```moonbit\nfn sum(x : Int) -> Int {\n  let mut result = 0\n  for i = 1; i <= x; i = i + 1 {\n    result += x\n    result %= 11451419\n    result -= 2\n    result = result | 1\n    result *= 19190\n    result %= 11451419\n  }\n  result\n}\n\nlet criterion = Criterion::new()\ncriterion.add(Task::new(\"sum\", fn() { sum(10000000) |> ignore }, count=100))\n```\n\nYou need to specify a name and a function to test for each task, with an optional parameter being the number of times it will be executed. In statistical experience, the higher the number of times, the more accurate the results will be, and the default value for the number of times is 10.\n\nNext, you can run these testsï¼š\n\n```moonbit\nlet result=criterion.run()\nprintln(result[\"sum\"])\n```\n\nThe return type is Map[String, TaskResult], which indexes the results of each run by name. Additionally, TaskResult implements the show trait, so it can be directly output.\n\nThe following is a detailed definition of Task/TaskResult:\n\n```moonbit\nstruct Task {\n  name : String // The name of the task\n  f : () -> Unit // The tested function\n  count : Int // Number of tests conducted\n}\n\nstruct TaskResult {\n  task : Task // Task corresponding to the result\n  average : Double // Average execution time\n  max : Double // Maximum execution time per execution\n  min : Double // Minimum execution time per execution\n}\n```\n",
  "package_data": "moonbitlang/x/benchmark/package_data.json",
  "source_files": [ "benchmark.mbt", "types.mbt" ]
}